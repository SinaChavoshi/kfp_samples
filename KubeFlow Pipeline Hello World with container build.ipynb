{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Install Pipeline SDK - This only needs to be ran once in the enviroment. \n",
    "# you can find the latest package @ https://github.com/kubeflow/pipelines/releases\n",
    "\n",
    "#KFP_PACKAGE = 'https://storage.googleapis.com/ml-pipeline/release/0.1.21/kfp.tar.gz'\n",
    "#!pip3 install $KFP_PACKAGE --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello world with KubeFlow Pipelines \n",
    "\n",
    "Welcome to your first step with KubeFlow Pipelines(KFP). In this notebook, we will demo: \n",
    "\n",
    "* Defining a KubeFlow pipeline with Python KFP SDK\n",
    "* Creating an experiment and submitting pipelines to KFP run time enviroment using the KFP SDK \n",
    "\n",
    "Reference documentation: \n",
    "* https://www.kubeflow.org/docs/pipelines/sdk/build-component/\n",
    "* https://www.kubeflow.org/docs/pipelines/sdk/sdk-overview/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Set your output and project. !!!Must Do before you can proceed!!!\n",
    "EXPERIMENT_NAME = 'Hellow world!'\n",
    "PROJECT_NAME =  'chavoshi-dev-2'                      #'Your-Gcp-Project-Name'\n",
    "OUTPUT_DIR = 'gs://chavoshi-dev-mlpipeline'          # A path for asset outputs\n",
    "BASE_IMAGE='tensorflow/tensorflow:1.11.0-py3'         # Based image used in various steps of the pipeline\n",
    "TARGET_IMAGE='gcr.io/%s/pusher:latest' % PROJECT_NAME # Target image that will include our final code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Experiment in the Pipeline System\n",
    "\n",
    "Pipeline system requires an \"Experiment\" to group pipeline runs. You can create a new experiment, or call client.list_experiments() to get existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that this notebook should be running in JupyterHub in the same cluster as the pipeline system.\n",
    "# Otherwise it will fail to talk to the pipeline system.\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.gcp import use_gcp_secret\n",
    "from kubernetes import client as k8s_client\n",
    "from kfp import compiler\n",
    "from kfp import notebook\n",
    "from kfp import components as comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.python_component(\n",
    "    name='add_op',\n",
    "    description='adds two numbers',\n",
    "    base_image=BASE_IMAGE  # note you can define the base image here, or during build time. \n",
    ")\n",
    "def add(a: float, b: float) -> float:\n",
    "    '''Calculates sum of two arguments'''\n",
    "    print(a)\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Pipeline Step With the Above Function(Note: run either of the two options below)\n",
    "#### Option One: Specify the dependency directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-20 14:02:12:INFO:Build an image that is based on tensorflow/tensorflow:1.11.0-py3 and push the image to gcr.io/chavoshi-dev-2/pusher:latest\n",
      "2019-06-20 14:02:12:INFO:Checking path: gs://chavoshi-dev-mlpipeline...\n",
      "2019-06-20 14:02:12:INFO:Generate entrypoint and serialization codes.\n",
      "2019-06-20 14:02:12:INFO:Generate build files.\n",
      "2019-06-20 14:02:13:INFO:Start a kaniko job for build.\n",
      "2019-06-20 14:02:13:INFO:Cannot Find local kubernetes config. Trying in-cluster config.\n",
      "2019-06-20 14:02:13:INFO:Initialized with in-cluster config.\n",
      "2019-06-20 14:02:18:INFO:5 seconds: waiting for job to complete\n",
      "2019-06-20 14:02:23:INFO:10 seconds: waiting for job to complete\n",
      "2019-06-20 14:02:28:INFO:15 seconds: waiting for job to complete\n",
      "2019-06-20 14:02:33:INFO:20 seconds: waiting for job to complete\n",
      "2019-06-20 14:02:38:INFO:25 seconds: waiting for job to complete\n",
      "2019-06-20 14:02:43:INFO:30 seconds: waiting for job to complete\n",
      "2019-06-20 14:02:48:INFO:35 seconds: waiting for job to complete\n",
      "2019-06-20 14:02:53:INFO:40 seconds: waiting for job to complete\n",
      "2019-06-20 14:02:58:INFO:45 seconds: waiting for job to complete\n",
      "2019-06-20 14:03:03:INFO:50 seconds: waiting for job to complete\n",
      "2019-06-20 14:03:08:INFO:55 seconds: waiting for job to complete\n",
      "2019-06-20 14:03:13:INFO:60 seconds: waiting for job to complete\n",
      "2019-06-20 14:03:18:INFO:65 seconds: waiting for job to complete\n",
      "2019-06-20 14:03:18:INFO:Kaniko job complete.\n",
      "2019-06-20 14:03:19:INFO:Build component complete.\n"
     ]
    }
   ],
   "source": [
    "from kfp import compiler\n",
    "\n",
    "# The return value \"DeployerOp\" represents a step that can be used directly in a pipeline function\n",
    "add_op = compiler.build_python_component(\n",
    "    component_func=add,\n",
    "    staging_gcs_path=OUTPUT_DIR,\n",
    "    dependency=[kfp.compiler.VersionedDependency(name='google-api-python-client', version='1.7.0')],\n",
    "    base_image=BASE_IMAGE,\n",
    "    target_image=TARGET_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option Two: build a base docker container image with both tensorflow and google api client packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-20 13:54:18:INFO:Checking path: gs://chavoshi-dev-mlpipeline...\n",
      "2019-06-20 13:54:18:INFO:Generate build files.\n",
      "2019-06-20 13:54:19:INFO:Start a kaniko job for build.\n",
      "2019-06-20 13:54:19:INFO:Cannot Find local kubernetes config. Trying in-cluster config.\n",
      "2019-06-20 13:54:19:INFO:Initialized with in-cluster config.\n",
      "2019-06-20 13:54:24:INFO:5 seconds: waiting for job to complete\n",
      "2019-06-20 13:54:29:INFO:10 seconds: waiting for job to complete\n",
      "2019-06-20 13:54:29:INFO:Kubernetes job failed.\n",
      "2019-06-20 13:54:29:INFO:Kaniko job complete.\n",
      "2019-06-20 13:54:29:INFO:Build image complete.\n"
     ]
    }
   ],
   "source": [
    "%%docker {TARGET_IMAGE} {OUTPUT_DIR}\n",
    "FROM {BASE_IMAGE}\n",
    "RUN pip3 install google-api-python-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the base docker container image is built, we can build a \"target\" container image that is base_image plus the python function as entry point. The target container image can be used as a step in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-20 13:55:30:INFO:Build an image that is based on tensorflow/tensorflow:1.11.0-py3 and push the image to gcr.io/chavoshi-dev-2/pusher:latest\n",
      "2019-06-20 13:55:30:INFO:Checking path: gs://chavoshi-dev-mlpipeline...\n",
      "2019-06-20 13:55:30:INFO:Generate entrypoint and serialization codes.\n",
      "2019-06-20 13:55:30:INFO:Generate build files.\n",
      "2019-06-20 13:55:30:INFO:Start a kaniko job for build.\n",
      "2019-06-20 13:55:30:INFO:Cannot Find local kubernetes config. Trying in-cluster config.\n",
      "2019-06-20 13:55:30:INFO:Initialized with in-cluster config.\n",
      "2019-06-20 13:55:35:INFO:5 seconds: waiting for job to complete\n",
      "2019-06-20 13:55:40:INFO:10 seconds: waiting for job to complete\n",
      "2019-06-20 13:55:45:INFO:15 seconds: waiting for job to complete\n",
      "2019-06-20 13:55:50:INFO:20 seconds: waiting for job to complete\n",
      "2019-06-20 13:55:55:INFO:25 seconds: waiting for job to complete\n",
      "2019-06-20 13:56:00:INFO:30 seconds: waiting for job to complete\n",
      "2019-06-20 13:56:06:INFO:35 seconds: waiting for job to complete\n",
      "2019-06-20 13:56:11:INFO:40 seconds: waiting for job to complete\n",
      "2019-06-20 13:56:16:INFO:45 seconds: waiting for job to complete\n",
      "2019-06-20 13:56:21:INFO:50 seconds: waiting for job to complete\n",
      "2019-06-20 13:56:26:INFO:55 seconds: waiting for job to complete\n",
      "2019-06-20 13:56:26:INFO:Kaniko job complete.\n",
      "2019-06-20 13:56:26:INFO:Build component complete.\n"
     ]
    }
   ],
   "source": [
    "from kfp import compiler\n",
    "\n",
    "# The return value \"DeployerOp\" represents a step that can be used directly in a pipeline function\n",
    "add_op = compiler.build_python_component(\n",
    "    component_func=add,\n",
    "    staging_gcs_path=OUTPUT_DIR,\n",
    "    target_image=TARGET_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a pipeline using this component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "@dsl.pipeline(\n",
    "   name='Calculation pipeline',\n",
    "   description='A toy pipeline that performs arithmetic calculations.'\n",
    ")\n",
    "def calc_pipeline(\n",
    "   a='a',\n",
    "   b='7',\n",
    "   c='17',\n",
    "):\n",
    "    #Passing pipeline parameter and a constant value as operation arguments\n",
    "    add_task = add_op(a, 4) #Returns a dsl.ContainerOp class instance. \n",
    "    \n",
    "    #You can create explicit dependancy between the tasks using xyz_task.after(abc_task)\n",
    "    add_2_task = add_op(a, b)\n",
    "    \n",
    "    add_3_task = add_op(add_task.output, add_2_task.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complie the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = calc_pipeline\n",
    "pipeline_filename = pipeline_func.__name__ + '.pipeline.zip'\n",
    "import kfp.compiler as compiler\n",
    "compiler.Compiler().compile(pipeline_func, pipeline_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit the pipeline for execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-20 14:09:05:INFO:Creating experiment Hellow world!.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/571db2d0-a74a-4170-a68c-42f2498819bb\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Get or create an experiment and submit a pipeline run\n",
    "import kfp\n",
    "client = kfp.Client()\n",
    "experiment = client.create_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/kfp/_client.py:194: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  return yaml.load(f)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/11bb7c28-9366-11e9-aeee-42010a80014f\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Specify pipeline argument values\n",
    "arguments = {'a': '7', 'b': '8'}\n",
    "\n",
    "#Submit a pipeline run\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments)\n",
    "\n",
    "#This link leads to the run information page. \n",
    "#Note: There is a bug in JupyterLab that modifies the URL and makes the link stop working"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
